---
title: "Estimating error associated with analyzing an aggregate dataset containing WG and WS CDI forms"
author: "Chris Cox"
format: html
bibliography: vocab-reporting.bib
csl: apa.csl
---

## The problem

The CDI has multiple versions, each designed for use with children at different developmental stages. In particular, the form tailored for children at the earliest stages of language development (Form 1, Words and Gestures; WG) includes a vocabulary checklist consisting of 395 words (198 nouns, 56 verbs, 37 adjectives, 37 function words, and 67 "other"), while the checklist included with the next form (Form 2, Words and Sentences; WS) consists of 680 words (312 nouns, 103 verbs, 63 adjectives, 102 function words, and 100 "other"). Thus, comparing vocabulary size and composition of children assessed with WG and WS forms may not be appropriate.

## Considerations

However, children do not develop their vocabularies at random. The 395 words on the WG checklist were chosen because they are among the earliest that a child will learn. In general, smaller vocabularies will likely be assessed more accurately than larger ones. In other words, the choice to assess a child with WG or WS when their vocabulary is small is inconsequential in terms of checklist coverage (although it does matter for other reasons).

Arunachalam et al [-@Arunachalam2021] studied the reliability of responses by comparing WG and WS forms administered at the same timepoint. There were 83 WG-WS pairs acquired for the study, and the children were between 14 to 43 months (mean age = 27 months, SD = 4.4 months) at the time of data collection. They reported 90% agreement across the two forms (ASD group: 89%, TD group: 90%), with nouns being more consistently reported (10%; ASD group: 10%, TD group: 9%) than verbs (14%; ASD group: 15%, TD group: 12%).
 
Interestingly, they note that "inconsistencies were more likely to take the form of reporting more knowledge on MCDI 1 than on MCDI 2 ($\chi^2(1) = 94.5$, $p < .001$): 60% of the inconsistencies were such that children were reported to "say" the word on the MCDI 1 form and left blank (indicating the child does not say the word) on the MCDI 2 form, with the remaining 40% having the child reported as either only "understanding" the word on the MCDI 1 form or left blank (indicating the child does not say the word) and checked as an indicator of saying the word on the MCDI 2 form." They speculate that the proportion of words a parent is able to report a child knows may influence their thinking about their child's abilities, and lead to a corresponding upward or downward bias.

In short, there are many potential sources of bias when considering parent report, and how accurately a vocabulary is estimated may be influenced by the form that is administered. There is no perfect assessment, and some measurement error must be tolerated. The following analyses attempt to quantify the error associated with under-sampling vocabulary with the WG form as a function of vocabulary size.
 

## Approach

In our current dataset, the WS form was administered 342 times to children with vocabularies of 395 words or less. Also, every word on the WG form has a close analog on the WS form (one important difference being that, on the WG form, "in" and "inside" are separate words while the WS form has "inside/in"). Thus, we can identify the "WG-set" of words embedded within the WS form. For each of these 342 WS administrations to children with 395 words or less, we can tally the number of words they produce and how many of those words would have been recorded if they had instead been administered the WG form.

The predicted errors we ultimately generate do not take into account any potential response biases, but rather assume that caregivers would have reported exactly the same words had they been administered the WG form instead of the WS form (among the words appear on both forms, of course).


## Load required packages and data

```{r}
library(dplyr, warn.conflicts = FALSE)
library(purrr)
library(tidyr)
library(readr)
library(ggplot2)

meta <- readRDS("data/cdi-item-data.rds")
str(meta)

asd_cdi <- readRDS("data/cdi-instrument-data-asd.rds")
str(asd_cdi)
```

## Summarize instrument data

We will summarize each administration in terms of:

  1. Number of words produced overall.
  2. Number of words produced within each of 5 lexical classes.
  3. Number of words produced that fall within the WG-set of words, overall ...
  4. ... and by lexical class.

```{r count}
asd_cdi_counts <- asd_cdi |>
    group_by(subjectkey, nproduced, sex, group, interview_age, form) |>
    summarize(
        nproduced_ws_total = sum(produced),
        nproduced_ws_noun  = sum(produced & lexical_class == "nouns"),
        nproduced_ws_verb  = sum(produced & lexical_class == "verbs"),
        nproduced_ws_adj   = sum(produced & lexical_class == "adjectives"),
        nproduced_ws_func  = sum(produced & lexical_class == "function_words"),
        nproduced_ws_other = sum(produced & lexical_class == "other"),
        nproduced_wg_total = sum(on_WG & produced),
        nproduced_wg_noun  = sum(on_WG & produced & lexical_class == "nouns"),
        nproduced_wg_verb  = sum(on_WG & produced & lexical_class == "verbs"),
        nproduced_wg_adj   = sum(on_WG & produced & lexical_class == "adjectives"),
        nproduced_wg_func  = sum(on_WG & produced & lexical_class == "function_words"),
        nproduced_wg_other = sum(on_WG & produced & lexical_class == "other"),
        .groups = "drop"
    ) |>
    select(-nproduced) |>
    pivot_longer(
        cols = starts_with("nproduced_"),
        names_to = c("wordset", "lexical_class"),
        names_prefix = "nproduced_",
        names_sep = "_",
        values_to = "nproduced"
    ) |>
    mutate(
        lexical_class = factor(
            lexical_class,
            levels = c("noun", "verb", "adj", "func", "other", "total")
        )
    )

```

## Summarize item data

It will be helpful to standardize our data by the number of words on each form, and by lexical class within each form. We compute these counts now for later use.

```{r max by class}
count_lexical_class <- bind_rows(
    WG =  bind_rows(
        count(filter(meta, on_WG), lexical_class),
        tibble(lexical_class = "total", n = sum(meta$on_WG))
    ),
    WS = bind_rows(
        count(meta, lexical_class),
        tibble(lexical_class = "total", n = nrow(meta))
    ),
    .id = "form"
) |>
    mutate(
        lexical_class = factor(
            lexical_class,
            c("nouns", "verbs", "adjectives", "function_words", "other", "total"),
            c("noun", "verb", "adj", "func", "other", "total")
        )
    )

```

The maximum number of words that could have been missed by administering the WG form is the number of words on the WS form minus the number on the WG form. We prepare this difference now for later use.

```{r max count diff}
count_lexical_class_diff <- count_lexical_class |>
    pivot_wider(id_cols = lexical_class, names_from = form, values_from = n) |>
    mutate(diff = WS - WG)

knitr::kable(count_lexical_class_diff)
```


## Shared configuration

```{r config}
line_widths <- c(total = 2, noun = .7, verb = .7, adj = .7, func = .7, other = .7)
```

## Number of administrations

All WG and WS forms in this data set were administered to autistic children. Administrations of the WS form are split into those reporting $\le395$ words and those reporting $>395 words$, since our analyses of the WS data will be restricted to those where the estimated vocabulary size could, in principle, be accommodated by the WG form.

```{r number of administrations}
asd_cdi_counts |>
    filter(lexical_class == "total", wordset == "ws") |>
    distinct(subjectkey, sex, group, interview_age, form, nproduced) |>
    mutate(le_395 = nproduced <= 395) |>
    count(group, form, le_395) |>
    knitr::kable(col.names = c("group", "form", "&leq;395", "n"))
```

## Vocabulary size distributions

The potential for under-sampling a child's vocabulary with the WG form increases with vocabulary size. Thus, should first consider the vocabulary sizes as assessed by our WG and WS administrations.

### Histogram of raw counts

This figure reports raw counts in bins of 20. The y-axis is held constant over rows.

```{r histogram of raw counts}
asd_cdi_counts |>
    filter(wordset == "ws") |>
    ggplot(aes(x = nproduced, fill = lexical_class)) +
        geom_histogram(binwidth = 20) +
        facet_grid(lexical_class ~ form, scales = "free_x") +
        theme_bw(base_size = 12) +
        theme(legend.position = "none")
```


### Histogram of scaled counts

By plotting density instead of counts on the y-axis, we correct for the fact that there are more WS than WG administrations in the data set and make the visualizations more comparable between forms (i.e., columns). The y-axis is also free to vary by row in this figure, which emphasizes the shape of the distributions, but means that bar height is not comparable between rows.

```{r histogram of scaled counts}
asd_cdi_counts |>
    filter(wordset == "ws") |>
    left_join(rename(count_lexical_class, lexical_class_max = n)) |>
    ggplot(aes(x = nproduced, fill = lexical_class)) +
        geom_histogram(aes(y = after_stat(density)), binwidth = 20) +
        facet_grid(lexical_class ~ form, scales = "free") +
        theme_bw(base_size = 12) +
        theme(legend.position = "none")
```

### Overlayed density curves

For these plots, the x-axis is scaled to a proportion of total possible given the form and lexical class. This allows for the shapes of the density curves to be overlaid.

```{r overlayed density curves}
asd_cdi_counts |>
    filter(wordset == "ws") |>
    left_join(
        rename(count_lexical_class, lexical_class_max = n),
        by = join_by(form, lexical_class)
    ) |>
    mutate(
        nproduced = nproduced / lexical_class_max
    ) |>
    ggplot(aes(x = nproduced, color = lexical_class, linewidth = lexical_class)) +
        geom_density() +
        scale_linewidth_manual(values = line_widths) +
        facet_wrap(vars(form), scales = "free_x") +
        theme_bw(base_size = 12)
```

### Summary of vocabulary distributions

The children assessed with the WG form in this data set tended to have very small vocabularies. Function words and adjectives are underrepresented in these vocabularies, and nouns and "other" words are somewhat over-represented.

```{r wg quantiles}
asd_cdi_counts |>
    filter(lexical_class == "total", wordset == "ws", form == "WG") |>
    pull(nproduced) |>
    quantile(probs = c(.5, .75, .9))
```


## Model error rate using WS administrations

We will now narrow our focus to the 342 WS forms administered to children with vocabularies estimated to be $\le395$ words. We can plot their total vocabulary size against the number of those words that belong to the WG-set. This gives some sense of how much their vocabulary size would have been underestimated if they had been assessed with the WG form.


```{r scatter plot}
asd_cdi_counts |>
    pivot_wider(
        id_cols = c(subjectkey:form, lexical_class),
        names_from = wordset,
        names_prefix = "n_",
        values_from = nproduced
    ) |>
    left_join(
        rename(count_lexical_class, lexical_class_max = n),
        by = join_by(form, lexical_class)
    ) |>
    left_join(
        select(filter(count_lexical_class, form == "WG"),
               lexical_class,
               lexical_class_max_wg = n
        ),
        by = join_by(lexical_class)
    ) |>
    filter(
        n_ws <= lexical_class_max_wg,
        form == "WS"
    ) |>
    ggplot(aes(x = n_wg, y = n_ws, color = lexical_class)) +
        geom_point(color = "darkgrey") +
        geom_smooth(formula = y ~ s(x, bs = "cs"), method = "gam") +
        geom_abline(slope = 1, intercept = 0) +
        facet_wrap(vars(lexical_class), scales = "free") +
        theme_bw() +
        theme(legend.position = "none") +
        xlab("Number of words produced in WG-set") +
        ylab("Total number of words produced")

```

## Fit linear models to relate WG-set to full vocabulary size

The figures above used a Generalized Additive Model (GAM) to fit a non-linear curve describing the relationship between the number of words on the WG-set and the total vocabulary size. While this does reveal some potential non-linear relationships, the increase in error as a function of vocabulary size is approximately linear.

For simplicity, we model the relationship as linear, and rectify the predictions so that no predicted WS vocabulary size can be less than the vocabulary size as assessed on the WG form.

By fitting linear models to this WS data, we can predict what a child's score on the WS form would have been given their score on the WG form. In this way, we estimate the amount of error the choice of form may have introduced by under-sampling the child's vocabulary knowledge.


### Fit models
```{r fit GAMs}
tmp <- asd_cdi_counts |>
    pivot_wider(
        id_cols = c(subjectkey:form, lexical_class),
        names_from = wordset,
        names_prefix = "n_",
        values_from = c(nproduced)
    ) |>
    left_join(
        rename(count_lexical_class, lexical_class_max = n),
        by = join_by(form, lexical_class)
    ) |>
    left_join(
        select(filter(count_lexical_class, form == "WG"),
               lexical_class,
               lexical_class_max_wg = n
        ),
        by = join_by(lexical_class)
    ) |>
    group_by(lexical_class)

models <- tmp |>
    filter(
        n_ws <= lexical_class_max_wg,
        form == "WS"
    ) |>
    group_split() |>
    map(~ lm(n_ws ~ n_wg, data = .x))
    #map(~ mgcv::gam(n_ws ~ s(n_wg, bs = "cs"), data = .x))

names(models) <- tmp |> group_keys() |> pull(lexical_class)
```

### Predict WS vocab size from WG

```{r predict ws vocab size}
asd_cdi_counts_wg <- asd_cdi_counts |>
    filter(form == "WG", wordset == "wg") |>
    select(-wordset) |>
    group_by(lexical_class) |>
    mutate(
        pred_wg_to_ws_raw = c(predict(models[[lexical_class[1]]], tibble(n_wg = nproduced))),
        pred_wg_to_ws = if_else(pred_wg_to_ws_raw < nproduced, nproduced, pred_wg_to_ws_raw),
        pred_error = pred_wg_to_ws - nproduced
    ) |>
    left_join(
        select(count_lexical_class_diff, lexical_class, lexical_class_diff = diff, n_wg = WG),
        by = join_by(lexical_class)
    ) |>
    group_by(lexical_class) |>
    mutate(
        pred_error_rate = pred_error / lexical_class_diff,
        pproduced = nproduced / n_wg
    ) |>
    ungroup()

```


### Plot distribution of predicted error rates by lexical class

After standardizing errors by the maximum possible (i.e., the difference in the number of words on the WS and WG forms), we can visualize the error distributions as density plots. For most of the WG forms administered, the predicted error is small. This makes sense, given that WG vocabularies were typically small.


```{r}
asd_cdi_counts_wg |>
    group_by(lexical_class) |>
    ggplot(aes(x = pred_error_rate, color = lexical_class, linewidth = lexical_class)) +
        geom_density() +
        scale_linewidth_manual(values = line_widths) +
        facet_wrap(vars(form)) +
        theme_bw() +
        xlab("Predicted Error Rate")

```


### Cumulitive density plot

In this plot, the x-axis is the predicted error rate and the y-axis is the proportion of WG administrations with that error rate or less.

```{r}
asd_cdi_counts_wg |>
    ggplot(aes(x = pred_error_rate, color = lexical_class, linewidth = lexical_class)) +
        #geom_density() +
        stat_ecdf(geom = "step") +
        scale_linewidth_manual(values = line_widths) +
        scale_y_continuous(breaks = seq(0,1,.2)) +
        facet_wrap(vars(form)) +
        theme_bw() +
        xlab("Predicted Error Rate") +
        ylab("Empirical Cumulative Density")


```


## Overlay predicted error rate on density curve

Finally, to visualize the predicted error rate as a function of vocabulary size and the probability of observing that vocabulary size, we apply a binned color scheme to the density curves.

```{r}

kernal_density_estimate <- function(x, t, bw = bw.nrd0, h = bw(x)) {
    if (length(t) == 1) {
        mean(dnorm(t, mean = x, sd = h))
    } else {
        t <- rep(t, each = length(x))
        colMeans(matrix(dnorm(t, mean = x, sd = h), nrow = length(x)))
    }
}

asd_cdi_counts_wg <- asd_cdi_counts_wg |>
    group_by(lexical_class) |>
    mutate(
        kde = kernal_density_estimate(pproduced, pproduced)
    ) |>
    ungroup()

asd_cdi_counts_wg |>
    ggplot(aes(x = pproduced, y = kde, color = pred_error_rate)) +
        geom_line(linewidth = 2, lineend = "round") +
        scale_color_binned(type = "viridis") +
        theme_bw() +
        xlab("Proportion of Words Produced") +
        facet_wrap(vars(lexical_class))

```
